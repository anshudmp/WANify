[PLUGIN_CONFIGS]
#username - The username on cloud instances with which they can be accessed. For instance, ubuntu, ec2-user, etc.
username = ec2-user

# NUM_DATACENTERS defines the maximum number of data centers against which the training dataset is collected
NUM_DATACENTERS = 8

# RUN_MODE defines what to capture in BWs measurements: 'STATIC' vs 'DYNAMIC' vs 'BOTH'. Snapshots are generated only in the DYNAMIC or BOTH flow.
RUN_MODE = DYNAMIC

# runForDurations defines what number of seconds each sample in the generated dataset will run. For example, <2, 15, 30> means three files will be generated for each dataset: dynamicX_2.json, dynamicX_15.json and dynamicX_30.json. dynamicX_2.json are the dynamic runtime BWs when iPerf monitoring is run for 2 seocnds and so on. By default, runForDurations contains the value 1, which is the base criteria for predicting accurate runtime BWs.
runForDurations=20

# runInterval defines the frequency at which the montoring should be run; 'once' denotes running the monitoring tool just once, '1s' or '2s' denote at the interval of 1 second or 2 seconds, 1d' or '2d' denote at the interval of 1 day or 2 days, '1w' or '2w' denote at the interval of 1 week or 2 week, '1m' or '2m' denote at the interval of 1 month of 2 months, respectively. Note that the tool considers each month as 30 days based on the average number of days in a month
runInterval = once

# model output path. This is the directory where the trained model will be saved. This key is only honored if buildModel below is set to True. Don't append '/' at the end.
modelOutputPath = model

# model training flag
buildModel = False

# only train the prediction model training using the datasetPath directory defined below
trainModeOnly = False

# datasetPath defines the output directory to store the generated datasets. Each dataset sample is contained in 2 files: static.json contains the static pair-wise bandwidths and dynamic.json contains the dynamic bandwidth values between interacting datacenters. If the path does not exist, then the script will create it. Don't append '/' at the end.
datasetPath = datasets

# Base port for monitoring bandwidth. Ensure that at least x number of ports are successively available, where x is the number of participating instances for bandwidth measurement.
basePort = 5000

# dataset size - this defines the number of samples to be generated. Each sample contains static*.json and dynamic*.json. Default value is 10!
datasetSize = 601

# resumeDatasetFromIndex - this flag resumes the dataset generation from given index. It is useful if there was some error in between the previous run and therefore, new samples need to continue from where it last ended. This can also be used for incremental dataset generation. Default value is 1, which means begin from the start. Make sure that no file with index 1 through "datasetSize" exists in the datasetPath directory, otherwise the code will throw an error.
resumeDatasetFromIndex = 601

# maxRetries in the event of failure, error or any other exception.
maxRetries = 3

# Private key path for accessing the instances. This is needed to send monitors and collect bandwidth information.
privateKeyPath = keys/ec2.pem

isPrivateIPUsageEnabled = True

# If debugging is enabled, then additional logs will be printed to trace the control flow in respective scripts.
isDebugEnabled = True

# If status progress message is enabled, then the system will print status messages at important stages. This can be used by end-user to actively track the completion of the samples generation.
statusProgressMsgEnabled = True

# spawn instances from scratch. If not using these, comment them out.
[LAUNCH_CONFIGS]
AMIs = {"us-east-1": "ami-06511c8846aca8b31", "us-west-1": "ami-03bd02098685a569d", "ap-south-1": "ami-0c83fc9e94629c55e", "ap-southeast-1": "ami-020e6327ceeb3546a", "ap-southeast-2": "ami-0292f81b7e001db54", "ap-northeast-1": "ami-090c4930e5920d2e1", "eu-west-1": "ami-0adecc01fe81e8062", "sa-east-1": "ami-03234eb733f2283e7"}

# subnets to use for each region
Subnets = {"us-east-1": "subnet-05b84b77c7c589c85", "us-west-1": "subnet-03c8adad4f91b0a2b", "ap-south-1": "subnet-00c867aa7c83c3d13", "ap-southeast-1": "subnet-023f4278e7588fad1", "ap-southeast-2": "subnet-050deab427c8bebae", "ap-northeast-1": "subnet-023cdd281b13d778c", "eu-west-1": "subnet-05128699bc809f550", "sa-east-1": "subnet-0d1c71f4a2e69290a"}

# This stores the indices of each region. Important for model training.
dcToIndexMap = {"us-east-1": 0, "us-west-1": 1, "ap-south-1": 2, "ap-southeast-1": 3, "ap-southeast-2": 4, "ap-northeast-1": 5, "eu-west-1": 6, "sa-east-1": 7}

instanceType = t3.nano

# Configs which need to be used for runtime throughput determination.
[PREDICT_CONFIGS]
#use public IPs here, since they are required to compute distance.
inUseIpsToProviderRegions = {"54.162.90.200": "aws~us-east-1", "13.52.253.174": "aws~us-west-1", "13.232.111.194": "aws~ap-south-1", "13.251.103.137": "aws~ap-southeast-1", "3.106.59.109": "aws~ap-southeast-2", "18.182.3.227": "aws~ap-northeast-1", "54.246.66.23": "aws~eu-west-1", "15.229.16.3": "aws~sa-east-1"}

# use public IPs here
ipToInstanceTypes = {"54.162.90.200": "t2.medium", "13.52.253.174": "t2.medium", "13.232.111.194": "e2.medium", "13.251.103.137": "e2.medium", "3.106.59.109": "e2.medium", "18.182.3.227": "e2.medium", "54.246.66.23": "t2.medium", "15.229.16.3": "t2.medium"}

# leave blank if private IPs are not being used
pubToPrivateIps = {"54.162.90.200": "172.31.94.153", "13.52.253.174": "172.32.88.142", "13.232.111.194": "172.33.36.175", "13.251.103.137": "172.34.93.84", "3.106.59.109": "172.35.94.124", "18.182.3.227": "172.36.95.236", "54.246.66.23": "172.37.83.1", "15.229.16.3": "172.38.40.78"}

# This stores the indices of the provider DCs. The regions should be consistent with the dcToIndexMap key defined before, since model is trained based on that key.
providerDCToIndexMap = {"aws~us-east-1": 0, "aws~us-west-1": 1, "aws~ap-south-1": 2, "aws~ap-southeast-1": 3, "aws~ap-southeast-2": 4, "aws~ap-northeast-1": 5, "aws~eu-west-1": 6, "aws~sa-east-1": 7}

# username for each provider; leave blank if multiple providers are not used
providerToUsername = {"aws": "ubuntu", "gcp": "anshudmp"}

# keyPath for each provider; leave blank if multiple providers are not used
providerToKeyPath = {"aws": "keys/ec2.pem", "gcp": "keys/google_compute_engine"}

# captureStatic defines whether the monitoring report should contain static pair-wise BWs. By default, monitoring captures 1s snapshots and determines runtime throughput.
captureStatic = False

# Flag to check if Global Optimizer is enabled.
greedyOptimization = True

# compareFrequency defines the interval at which dynamic BWs are captured for comparison with predicted BWs. If difference > errorThreshold, then the model is flagged for re-training. 
compareFrequency = 10

# compare snapshot and dynamic BWs based on compareFrequency above and report if number of DC pairs with error > 100 Mbps are higher than errorNumThreshold
errorNumThreshold = 10

# reportPath or report directory can contain multiple files based on compareFrequency and captureStatic. However, it will also contain status.txt, which says if current predictions are healthy or retraining is required.
reportPath = reports
